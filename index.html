<html>

<head>
    <meta charset="utf-8" />
    <title>TVL: A Touch, Vision, and Language Dataset for Multimodal Alignment</title>
    <link rel="shortcut icon" type="image/jpg" href="images/favicon.jpg"/>
    <meta content="TVL: A Touch, Vision, and Language Dataset for Multimodal Alignment" />
    <meta content="TVL: A Touch, Vision, and Language Dataset for Multimodal Alignment" property="og:title" />
    <meta content="TVL: A Touch, Vision, and Language Dataset for Multimodal Alignment" property="og:description" />
    <meta content="https://tactile-vlm.github.io/images/splash.png" property="og:image" />
    <meta content="TVL: A Touch, Vision, and Language Dataset for Multimodal Alignment" property="twitter:title" />
    <meta content="TVL: A Touch, Vision, and Language Dataset for Multimodal Alignment" property="twitter:description" />
    <meta content="https://tactile-vlm.github.io/images/splash.png" property="twitter:image" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"
        crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Varela+Round&display=swap"
        rel="stylesheet">
    <link href="style.css?v=v2" rel="stylesheet" type="text/css" />
</head>

<body>
    <div class="section">
        <div class="container">
            <div class="title-row">
                <h1 class="title main-title"
                    style="font-weight: 700; font-size: 50px; font-family: 'Varela Round',sans-serif; margin: 0">
                    TVL</h1>
                <h1 class="title main-title">A Touch, Vision, and Language Dataset for Multimodal Alignment</h1>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col1">
                    <a href="https://max-fu.github.io" target="_blank" class="author-text">Max (Letian) Fu<sup>1</sup></a>
                </div>
                <div class="base-col author-col1">
                    <a href="https://www.linkedin.com/in/gaurav-datta/" target="_blank" class="author-text">Gaurav Datta<sup>*</sup><sup>1</sup></a>
                </div>
                <div class="base-col author-col1">
                    <a href="https://qingh097.github.io/" target="_blank" class="author-text">Raven Huang<sup>*</sup><sup>1</sup></a>
                </div>
                <div class="base-col author-col1">
                    <div class="author-text" style="text-decoration:none">
                        Will Panitch<sup>*</sup><sup>1</sup>
                    </div>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col2">
                    <div class="author-text" style="text-decoration:none">
                        Jaimyn Drake<sup>*</sup><sup>1</sup>
                    </div>
                </div>
                <div class="base-col author-col2">
                    <a href="https://joeaortiz.github.io/" target="_blank" class="author-text">Joseph Ortiz<sup>2</sup></a>
                </div>
                <div class="base-col author-col2">
                    <a href="https://www.mustafamukadam.com/" target="_blank" class="author-text">Mustafa Mukadam<sup>2</sup></a>
                </div>
                <div class="base-col author-col2">
                    <a href="https://scholar.google.com/citations?user=p6DCMrQAAAAJ&hl=en/" target="_blank" class="author-text">Mike Lambeta<sup>2</sup></a>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col3">
                </div>
                <div class="base-col author-col3">
                    <a href="https://www.robertocalandra.com/" target="_blank" class="author-text">Roberto Calandra<sup>3,4</sup></a>
                </div>
                <div class="base-col author-col3">
                    <a href="https://goldberg.berkeley.edu" target="_blank" class="author-text">Ken Goldberg<sup>1</sup></a>
                </div>
                <div class="base-col author-col3">
                </div>
            </div>
            <div class="base-row affiliation-row">
                <div class="base-col affiliation-col">
                    <sup>1</sup>UC Berkeley
                </div>
                <div class="base-col affiliation-col">
                    <sup>2</sup>Meta AI Research
                </div>
                <div class="base-col affiliation-col">
                    <sup>3</sup>TU Dresden
                </div>
            </div>
            <div class="base-row affiliation-row">
                <div class="base-col affiliation-col-wide">
                    <sup>4</sup>The Centre for Tactile Internet with Human-in-the-Loop (CeTI)
                </div>
            </div>
            <div class="link-labels base-row" style="max-width: 500px">
                <div class="base-col icon-col"><a href="files/tvl.pdf" target="_blank"
                        class="link-block">
                        <i class="fa fas fa-file-text main-icon" style="font-size: 50px"></i>
                    </a>
                </div>
                <div class="base-col icon-col"><a href="https://github.com/Max-Fu/tvl" class="link-block">
                        <i class="fa fa-github main-icon" style="font-size: 55px"></i>
                    </a>
                </div>
                <div class="base-col icon-col"><a href="https://huggingface.co/datasets/mlfu7/Touch-Vision-Language-Dataset" class="link-block">
                        <i class="fa fa-database main-icon" style="font-size: 50px"></i>
                    </a>
                </div>
                <div class="base-col icon-col"><a href="https://huggingface.co/mlfu7/Touch-Vision-Language-Models" class="link-block">
                        <i class="fa fa-download main-icon" style="font-size: 55px"></i>
                    </a>
                </div>
                <div class="base-col icon-col"><a href="#citation" class="link-block">
                        <i class="fa fa-graduation-cap main-icon" style="font-size: 50px"></i>
                    </a>
                </div>
            </div>
            <div class="link-labels base-row" style="max-width: 500px">
                <div class="base-col icon-col"><a href="https://arxiv.org/abs/2401.14391" target="_blank"
                        class="no-underline">
                        <strong class="link-labels-text">Paper</strong></a>
                </div>
                <div class="base-col icon-col"><a href="https://github.com/Max-Fu/tvl" class="no-underline">
                        <strong class="link-labels-text">Code</strong></a>
                </div>
                <div class="base-col icon-col"><a href="https://huggingface.co/datasets/mlfu7/Touch-Vision-Language-Dataset" class="no-underline">
                        <strong class="link-labels-text">Dataset</strong></a>
                </div>
                <div class="base-col icon-col"><a href="https://huggingface.co/mlfu7/Touch-Vision-Language-Models" class="no-underline">
                        <strong class="link-labels-text">Models</strong></a>
                </div>
                <div class="base-col icon-col"><a href="#citation" class="no-underline">
                        <strong class="link-labels-text">Citation</strong></a>
                </div>
            </div>
            <h1 class="tldr">
                <b>TL;DR</b>: Multi-modal alignment made easy using GPT-4V pseudolabels.
            </h1>

            <div class="base-row add-top-padding">
                <div class="base-row add-top-padding">
                    <img class="img" src="images/splash.png" />
                </div>
                <h1 class="title">Overview</h1>
                <p class="paragraph">
                    We introduce the <b>Touch-Vision-Language (TVL)</b> dataset, which combines paired tactile and visual observations with both human-annotated and VLM-generated tactile-semantic labels.
                    We then leverage a contrastive learning approach to train a CLIP-aligned tactile encoder and finetune an open-source LLM for a tactile description task.
                    Our results show that incorporating tactile information allows us to significantly outperform state-of-the-art VLMs (including the label generating model) on a tactile understanding task.
                </p>
            </div>
            <div class="base-row add-large-top-padding">
                <h1 class="title">The TVL Dataset</h1>
                <div class="base-row add-top-padding">
                    <img class="img" src="images/sensor.png" />
                </div>
                <p class="paragraph add-top-padding">
                    We gather data using a handheld, 3D printed collection device.
                    Tactile data are collected using a DIGIT sensor: a compact, open-source tactile sensor that provides observations in the form of RGB images of a deformable internal surface.
                    Image data come from a Logitech BRIO webcam, positioned such that the tactile sensor and the point of contact are within its field of view.
                    The collected data are then temporally synchronized and labeled with language descriptions of the tactile sensations present to produce a dataset of aligned touch-vision-language examples.
                </p>
                <div class="base-row add-top-padding">
                    <img class="img" src="images/dataset.png" />
                </div>
                <p class="paragraph add-top-padding">
                    <b>The TVL Dataset</b> is then formed by combining two datasets: SSVTP (Kerr et al., 2023), a robot-collected dataset of 4,587 image-touch pairs, and HCT, our newly-collected dataset consisting of 39,154 synchronously captured in-the-wild image-touch pairs.
                    For the SSVTP dataset, we manually append image labels to the data (some examples are shown in the first row). For the HCT portion of the dataset, we prompt GPT-4V to label the dataset based on the visual inputs. Note that while GPT-4V is a very efficient visual labeler, it does occasionally fail to provide correct tactile labels (examples shown in row 4), especially when the contact patch is occluded by the sensor or when there is not sufficient visual information to estimate the tactile sensation. In total, this results in a dataset containing 43,741 in-contact image-touch pairs with open-vocabulary language labels.</p>
                <div class="base-row add-top-padding">
                    <img class="img" src="images/distribution.png" />
                </div>
            </div>
            <div class="base-row add-top-padding">
                <h1 class="title">Model Architecture and Training</h1>
                <div class="base-row add-top-padding">
                    <img class="img" src="images/method.png" />
                </div>
                <p class="paragraph">
                    <b>Encoder Alignment</b> begins by randomly initializing the tactile encoder as a Vision Transformer (ViT). 
                    During training, we take as input the tactile image, and use the embeddings from the representations of the paired language and vision examples after going through the respective encoders.
                    We remove the projectors from these two encoders such that the tactile encoder learns to directly project to the common CLIP latent space.
                    We test on three model sizes: ViT-Tiny (5.7M paraeters), ViT-Small (22M), and ViT-Base (86M) and notice that directly adopting the ImageBind training recipe leads to overfitting on our relatively small training dataset of 44K pairs of in-contact data. 
                    Instead, we find that leveraging data in which the tactile sensor is not in contact with a surface (background images, which we collect as part of each trajectory) can mitigate this overfitting problem and enhance tactile representation learning by improving visual data diversity.
                    Therefore, we also add an additional 10% of the training data where the sensor is <b>not</b> in contact and assign these examples a text label of “background”. 
                    To further increase the diversity of language labels, we also randomly shuffle and select a subset of the words in the tactile description for each image.
                </p>
                <div class="base-row add-top-padding">
                    <img class="img" src="images/results.png" />
                </div>
                <p class="paragraph">
                    For the <b>language generation task</b>, we pretrain on both the LLaVA Visual Instruct CC3M 595K subset and the TVL dataset. 
                    For the CC3M subset, we provide an empty tactile image to the tactile modality. Then, during finetuning, we use a combination of TVL, Alpaca, and the LLaVA Visual Instruct 150K datasets. 
                    The tactile-semantic labels from the TVL dataset are further parsed and fed into a series of different Q&A prompts to increase data diversity.
                </p>
            </div>
            <div class="base-row add-top-padding">
                <h1 class="title">Results</h1>
                <p class="paragraph">
                    We measure the performance of our model on a tactile-semantic understanding task, as well as the alignment of our tactile encoder across different modes. 
                </p>
                <div class="base-row add-large-top-padding">
                    <img class="img" src="images/table1.png" style="max-width:1000px; width: 75%" />
                </div>
                <p class="paragraph">
                    On our tactile-semantic classification task, we find all sizes of TVL-LLaMA outperform GPT-4V, suggesting that our trained models generalize beyond the small fraction of human labels provided as part of the dataset. 
                    On the other hand, other open-source VLMs perform worse than GPT-4V on our benchmark, likely due to the lack of attention to human tactility in their visual training data.
                    Furthermore, our results also suggest that direct tactile-language alignment is useful, as evidenced by the lower comparative score of SSVTP-LLaMA, which only aligns the tactile and visual modalities during pre-training.
                    All of these findings are statistically significant at the α = 0.05 level. 
                </p>
                <div class="base-row add-large-top-padding">
                    <img class="img" src="images/table2.png" style="max-width:800px; width: 50%" />
                </div>
                <p class="paragraph">
                    As we use OpenCLIP to encode image and language observations, the TVL encoder shares its vision-language accuracy scores with other OpenCLIP-based models. We therefore compare the tactile-vision accuracy of our encoder against SSVTP; because they train on a small dataset collected in a lab setting, their model performs well on the lab-collected subset of the data, but does not generalize well to the new “in-the-wild” portion of the dataset. Additionally, since the tactile encoder is aligned to the language descriptions of tactility, it also shows better tactile-text alignment than OpenCLIP’s vision-text alignment.
                </p>
                <div class="base-row add-large-top-padding">
                    <img class="img" src="images/exploded_sensor.png" style="max-width:800px; width: 50%" />
                </div>
            </div>

            <div class="citation add-large-top-padding">
                <h1 id="citation">Citation</h1>
                <p style="font-size: 16px">If you use this work or find it helpful, please consider citing our work.</p>
                <pre id="codecell">
@article{fu2024tvl,
    title={A Touch, Vision, and Language Dataset for Multimodal Alignment}, 
    author={Letian Fu and Gaurav Datta and Huang Huang and William Chung-Ho Panitch and Jaimyn Drake and Joseph Ortiz and Mustafa Mukadam and Mike Lambeta and Roberto Calandra and Ken Goldberg},
    journal={arXiv preprint arXiv:2402.13232},
    year={2024}
}
                </pre>
            </div>
        </div>
    </div>

    <p class="credit">Credit: The design of this project page references the project pages of <a
            href="https://www.matthewtancik.com/nerf">NeRF</a>, <a
            href="https://crossmae.github.io">CrossMAE</a>, <a
            href="https://github.com/DeepMotionEditing/DeepMotionEditing.github.io">DeepMotionEditing</a>, and <a
            href="https://www.lerf.io/">LERF</a>.</p>
</body>
